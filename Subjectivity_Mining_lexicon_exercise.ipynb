{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Subjectivity Mining: lexicon-based approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "lexicon_wiegand_path = \"/Users/demikruijer/Documents/Studie/Subjectivity Mining/Assignment 3/baseLexicon.txt\"\n",
    "lexicon_hurtlex_path = \"/Users/demikruijer/Documents/Studie/Subjectivity Mining/Assignment 3/hurtlex_EN.tsv\"\n",
    "lexicon_mol_path = \"/Users/demikruijer/Documents/Studie/Subjectivity Mining/Assignment 3/mol.csv\"\n",
    "test_set_path = \"/Users/demikruijer/Documents/Studie/Subjectivity Mining/Assignment 3/Subjectivity_mining_assignment_3_4_data/olid-test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets in frames\n",
    "df_wiegand = pd.read_csv(lexicon_wiegand_path, delimiter='\\t', header=None) \n",
    "df_hurtlex = pd.read_csv(lexicon_hurtlex_path, delimiter='\\t')\n",
    "df_mol = pd.read_csv(lexicon_mol_path)\n",
    "df_test = pd.read_csv(test_set_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create merged lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Wiegand lexicon\n",
    "df_wiegand.columns = ['Word', 'Hateful'] \n",
    "df_wiegand['Word'] = df_wiegand['Word'].str.split('_').str.get(0)\n",
    "df_wiegand_hateful = df_wiegand[df_wiegand['Hateful'] == True]\n",
    "\n",
    "# Drop NaNs for MOL lexicon\n",
    "df_mol.dropna(subset=['en-american-english'], inplace=True)\n",
    "df_mol = df_mol[df_mol['en-american-english'] != '0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge lexicons and drop duplicates\n",
    "df_merged = pd.concat([df_wiegand_hateful['Word'], df_mol['en-american-english'], df_hurtlex['lemma']], ignore_index=True)\n",
    "df_merged.drop_duplicates(inplace=True)\n",
    "df_merged.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiegand_lexicon = set(df_wiegand_hateful['Word'])\n",
    "hurtlex_lexicon = set(df_hurtlex['lemma'])\n",
    "mol_lexicon = set(df_mol['en-american-english'])\n",
    "merged_lexicon = set(df_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run lexicon-based analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function checks whether word from lexicon is present in string\n",
    "def check_words(input_str, lexicon):\n",
    "    return any(word in input_str for word in lexicon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis by checking if words in strings are present in lexicons\n",
    "df_analysis = df_test.copy()\n",
    "df_analysis['label wiegand'] = df_analysis['text'].apply(lambda x: check_words(x, wiegand_lexicon))\n",
    "df_analysis['label hurtlex'] = df_analysis['text'].apply(lambda x: check_words(x, hurtlex_lexicon))\n",
    "df_analysis['label mol'] = df_analysis['text'].apply(lambda x: check_words(x, mol_lexicon))\n",
    "df_analysis['label merged'] = df_analysis['text'].apply(lambda x: check_words(x, merged_lexicon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis['labels'].sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "545e036c4b32438aced1f6b3c8d38ca151d9c36189e05839cb0aa568fda70ddd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
